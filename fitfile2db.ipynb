{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FitFile2DB\n",
    "\n",
    "```markdown\n",
    "Author:         Maik 'Schrottie' Bischoff\n",
    "Decription:     Parse Garmin .fit files and write the data to a CSV file or database.\n",
    "Version:        0.6\n",
    "Date:           21.04.2023\n",
    "Requires:       [dtcooper/python-fitparse](https://github.com/dtcooper/python-fitparse)\n",
    "```\n",
    "### Change-/Versionlog:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <span style=\"font-size: 85%;font-family: monospace\">0.6:</span>\n",
    "        </td>\n",
    "        <td>\n",
    "            <ul style=\"font-size: 85%;font-family: monospace\">\n",
    "                <li>Added (simple) error logging function.</li>\n",
    "                <li>Extension of the database with an 'id' field (autoincrement, not null, primary key) and adjustment of the write function because this field does not exist in the dataframe..</li>\n",
    "                <li>Added MAX and AVG fields to the totals table. If there was multiple max fields, the highest value would taken, if there was multiple avg fields, the average of all these fields would taken and written to database</li>\n",
    "                <li>Added table fitfile_userdata with some userdata from fit file (height, weight *hihi*, sleep_time and so on)</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <span style=\"font-size: 85%;font-family: monospace\">0.5:</span>\n",
    "        </td>\n",
    "        <td>\n",
    "            <ul style=\"font-size: 85%;font-family: monospace\">\n",
    "                <li>Added function to use PostgreSQL instead of SQLite3.</li>\n",
    "                <li>Renamed project to FITFILE2DB.\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <span style=\"font-size: 85%;font-family: monospace\">0.4:</span>\n",
    "        </td>\n",
    "        <td>\n",
    "            <ul style=\"font-size: 85%;font-family: monospace\">\n",
    "                <li>Added function to read activity type</lI>\n",
    "                <li>Added function to read the totals of an activity and write them into a table. If some fields of new fit files are missing, the fields would be added to the table.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <span style=\"font-size: 85%;font-family: monospace\">0.3:</span>\n",
    "        </td>\n",
    "        <td>\n",
    "            <ul style=\"font-size: 85%;font-family: monospace\">\n",
    "                <li>Added function to convert mph to kph</li>\n",
    "                <li>Added a function for write data into database.\n",
    "                    <ul>\n",
    "                        <li>convert some fields to correct datatype</li>\n",
    "                        <li>check whether all fields from the current .fit-file exist in an existing table, if necessary updating the table with the new fields</li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <span style=\"font-size: 85%;font-family: monospace\">0.2:</span>\n",
    "        </td>\n",
    "        <td>\n",
    "            <ul style=\"font-size: 85%;font-family: monospace\">\n",
    "                <li>Added a function to recursively search a directory for .fit files to allow processing multiple files at the same time.</li>\n",
    "                <li>Adjusting the field label for the longitude (position_long --> position_lon) so that the field is recognized properly when the data is processed further (e.g. in ArcGIS Pro)</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <span style=\"font-size: 85%;font-family: monospace\">0.1:</span>\n",
    "        </td>\n",
    "        <td>\n",
    "            <ul style=\"font-size: 85%;font-family: monospace\">\n",
    "                <li>Basic function for processing a .fit file.</li>\n",
    "                <li>Converting the crude Garmin coordinate format (semicircles) into 'real' coordinates.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitparse\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "from sqlalchemy.orm import Session\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "fit_path = os.path.join(os.getcwd(), 'testdata')\n",
    "csv_path = os.path.join(os.getcwd(), 'testdata')\n",
    "sqlite_db = os.path.join(os.getcwd(), 'fitfile_test.db')\n",
    "err_log = os.path.join(os.getcwd(), 'error.log')\n",
    "use_db = True # If True, all data were written into database and no CSV would be generated\n",
    "db_type = \"SQLITE\" # Possible types are 'PGSQL' for PostgreSQL and 'SQLITE' for SQLite3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize error logger\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "date_format = '%d-%b-%y %H:%M:%S'\n",
    "logging.basicConfig(filename=err_log, format=log_format, level=logging.DEBUG, datefmt=date_format)\n",
    "\n",
    "# Configure stream handler for console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.ERROR)\n",
    "console_handler.setFormatter(logging.Formatter(log_format, datefmt=date_format))\n",
    "\n",
    "# log, log, log ... ;)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "### find_fit_files\n",
    "\n",
    "```markdown\n",
    "Walk through a dir to find files.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fit_files(directory):\n",
    "    try:\n",
    "        fit_files = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if is_fit_file(file):\n",
    "                    fit_files.append(os.path.join(root, file))\n",
    "        return fit_files\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in find_fit_files: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_fit_file\n",
    "\n",
    "```markdown\n",
    "Only if it end with fit, it is a fit! ;)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_fit_file(filename):\n",
    "    try:\n",
    "          return filename.lower().endswith('.fit')\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in is_fit_file: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semicircles_to_degree\n",
    "\n",
    "```markdown\n",
    "Convert crappy Garmin semicircles to useful degrees\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def semicircles_to_degree(semicircles):\n",
    "    try:\n",
    "        return semicircles * (180 / 2 ** 31)\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in semicircles_to_degree: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mph_to_kph\n",
    "\n",
    "```markdown\n",
    "Convert miles per hour to kilometers per hour\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mph_to_kph(speeds):\n",
    "    try:\n",
    "        if speeds is None:\n",
    "            return None\n",
    "        # Convert a single speed value to a list\n",
    "        if isinstance(speeds, (int, float)):\n",
    "            speeds = [speeds]\n",
    "        # Filtering the list to retain only valid speed values\n",
    "        speeds = [s for s in speeds if s is not None and not np.isnan(s)]\n",
    "        # Convert speeds to kph\n",
    "        speeds = [s * 1.609344 for s in speeds]\n",
    "        # Return the first value if it was originally a single value, otherwise return the list\n",
    "        return speeds[0] if len(speeds) == 1 else speeds\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in mph_to_kph: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_env_variables\n",
    "\n",
    "```markdown\n",
    "Load environment variables needed for the db connection\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_variables():\n",
    "    try:\n",
    "        # check if .env-Datei available\n",
    "        if not os.path.isfile('.env'):\n",
    "            raise FileNotFoundError(\"Die .env-Datei wurde nicht gefunden\")\n",
    "\n",
    "        # load env-data from .env\n",
    "        load_dotenv()\n",
    "        host = os.getenv('DB_HOST')\n",
    "        database = os.getenv('DB_NAME')\n",
    "        user = os.getenv('DB_USER')\n",
    "        password = os.getenv('USER_PASSWD')\n",
    "\n",
    "        # check if all variables are available\n",
    "        if not all([server, database, username, password]):\n",
    "            raise ValueError(\"One or more environment variables are missing!\")\n",
    "\n",
    "        return host, database, user, password\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in load_env_variables: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write_to_database\n",
    "\n",
    "```markdown\n",
    "Function to write things into sqlite db\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_database(df, table_name):\n",
    "    try:\n",
    "        # Establish database connection\n",
    "        if db_type == \"PGSQL\":\n",
    "            host, database, user, password = load_env_variables()\n",
    "            conn = psycopg2.connect(host, database, user, password)\n",
    "        elif db_type == \"SQLITE\":\n",
    "            conn = sqlite3.connect(sqlite_db)\n",
    "        else:\n",
    "                return\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get the columns of the table\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        table_info = cursor.fetchall()\n",
    "        table_columns = [tup[1] for tup in table_info]\n",
    "        \n",
    "        # Add missing columns to the table\n",
    "        for col in df.columns:\n",
    "            if col != 'id' and col not in table_columns:\n",
    "                cursor.execute(f\"ALTER TABLE {table_name} ADD COLUMN {col} TEXT\")\n",
    "                conn.commit()\n",
    "                table_columns.append(col)\n",
    "                \n",
    "        # Write DataFrame to database\n",
    "        for row in df.itertuples(index=False):\n",
    "            # Create a dictionary mapping column names to their values for the current row\n",
    "            row_dict = {col: getattr(row, col) for col in df.columns}\n",
    "            # Create a list of values in the same order as the columns in the table\n",
    "            values = [row_dict.get(col, '') for col in table_columns if col != 'id']\n",
    "            \n",
    "            # debug: if table_name == 'fitfile_totals' or table_name == 'fitfile_userdata':\n",
    "            # debug:      print(f\"INSERT INTO {table_name} ({','.join([col for col in table_columns if col != 'id'])}) VALUES ({','.join(['?']*len(values))})\", tuple(values))\n",
    "            cursor.execute(f\"INSERT INTO {table_name} ({','.join([col for col in table_columns if col != 'id'])}) VALUES ({','.join(['?']*len(values))})\", tuple(values))\n",
    "            conn.commit()\n",
    "\n",
    "        # Close database connection\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"Data successfully written to table {table_name} in database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in write_to_database: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_from_database\n",
    "\n",
    "```markdown\n",
    "Function to read some stuff from sqlite db\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_database(query):\n",
    "    try:\n",
    "        # Establish database connection\n",
    "        if db_type == \"PGSQL\":\n",
    "            host, database, user, password = load_env_variables()\n",
    "            conn = psycopg2.connect(host, database, user, password)\n",
    "        elif db_type == \"SQLITE\":\n",
    "            conn = sqlite3.connect(sqlite_db)\n",
    "        else:\n",
    "                return\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        requested_data = pd.read_sql_query(query, conn)\n",
    "\n",
    "        # Close connection and cursor\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        # Return requested data as a Pandas DataFrame\n",
    "        return requested_data\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in read_from_database: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_activity_type\n",
    "\n",
    "```markdown\n",
    "Which type of activity is recorded in actual file?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_type(fit_file):\n",
    "    try:\n",
    "        activity_type = None\n",
    "\n",
    "        # open the file\n",
    "        with fitparse.FitFile(fit_file) as fitfile:\n",
    "\n",
    "            # look for all records with type \"activity\"\n",
    "            for record in fitfile.get_messages(\"activity\"):\n",
    "\n",
    "                # read data field \"sport\" to get activity type\n",
    "                sport_field = record.get(\"sport\")\n",
    "                if sport_field:\n",
    "                    activity_type = sport_field.value\n",
    "\n",
    "        return activity_type\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in get_activity_type: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_totals\n",
    "\n",
    "```markdown\n",
    "Get all totals from fit file and calculate some of them\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_totals(filename):\n",
    "    try:\n",
    "        # Open the fit file\n",
    "        fitfile = fitparse.FitFile(filename)\n",
    "\n",
    "        # Create an empty list to hold the totals data\n",
    "        totals_data = []\n",
    "\n",
    "        # Get the serial number and product id of the device\n",
    "        serial_number = None\n",
    "        garmin_product = None\n",
    "        for record in fitfile.get_messages(\"file_id\"):\n",
    "            serial_number = record.get_value(\"serial_number\")\n",
    "            garmin_product = record.get_value(\"garmin_product\")\n",
    "            break\n",
    "\n",
    "        # Loop over all the messages in the FIT file\n",
    "        for record in fitfile.get_messages():\n",
    "            # Check if this is a \"session\" message\n",
    "            if record.name == \"session\":\n",
    "                # Get the activity type\n",
    "                activity_type = record.get_value(\"sport\")\n",
    "                activity_subtype = record.get_value(\"sub_sport\")\n",
    "\n",
    "                # Create an empty dictionary to hold the totals fields for this message\n",
    "                totals_fields = {\"activity_type\": activity_type, \"activity_subtype\": activity_subtype}\n",
    "\n",
    "                # Loop over all the fields in this message\n",
    "                for field in record:\n",
    "                    # Check if this field is a \"totals\" field\n",
    "                    if (field.name.startswith(\"total_\") or field.name.startswith(\"max_\") or field.name.startswith(\"avg_\")):\n",
    "                        try:\n",
    "                            # Try to add the field to the dictionary\n",
    "                            if field.name.startswith(\"max_\") and is_numeric_field(field):\n",
    "                                # If this is a max field, take the maximum value\n",
    "                                if field.name[4:] not in totals_fields or safe_max(field.value) > totals_fields[field.name[4:]]:\n",
    "                                    totals_fields[field.name[4:]] = safe_max(field.value)\n",
    "                            elif field.name.startswith(\"avg_\") and is_numeric_field(field):\n",
    "                                # If this is an avg field, add it to the total and count how many times it appears\n",
    "                                if field.name[4:] not in totals_fields:\n",
    "                                    totals_fields[field.name[4:]] = [field.value, 1]\n",
    "                                else:\n",
    "                                    totals_fields[field.name[4:]][0] += field.value\n",
    "                                    totals_fields[field.name[4:]][1] += 1\n",
    "                            else:\n",
    "                                # This is a total field, just add it to the dictionary\n",
    "                                totals_fields[field.name] = field.value\n",
    "                        except TypeError as te:\n",
    "                            # If there is a TypeError, skip this field\n",
    "                            print(f\"Skipping field {field.name} because of TypeError: {te}\")\n",
    "                            logger.warning(f\"Skipping field {field.name} because of TypeError: {te}\")\n",
    "\n",
    "                # Compute the average for all the avg fields\n",
    "                for key in totals_fields.keys():\n",
    "                    if isinstance(totals_fields[key], list):\n",
    "                        if totals_fields[key][1] > 0:\n",
    "                            totals_fields[key] = totals_fields[key][0] / totals_fields[key][1]\n",
    "                        else:\n",
    "                            totals_fields[key] = 0\n",
    "                    elif isinstance(totals_fields[key], (int, float)):\n",
    "                        totals_fields[key] = [totals_fields[key]]\n",
    "                    else:\n",
    "                        logger.warning(f\"Invalid type for {key}\")\n",
    "\n",
    "                # Add the totals fields for this message to the list\n",
    "                totals_data.append(totals_fields)\n",
    "\n",
    "        # Create a pandas dataframe from the totals data\n",
    "        df_totals = pd.DataFrame(totals_data)\n",
    "\n",
    "        # Check if the dataframe is empty\n",
    "        if df_totals.empty:\n",
    "            logger.warning(f\"Empty DataFrame for file {filename}\")\n",
    "            return None\n",
    "\n",
    "        # Get the base filename without path\n",
    "        filename = os.path.basename(filename)\n",
    "        # Extract the first part of the filename before the first underscore\n",
    "        name_part = filename.split('_')[0]\n",
    "        # Add a new column with the name part of the file name\n",
    "        df_totals.insert(0, \"activity_number\", name_part)\n",
    "        # Add the product name to the DataFrame\n",
    "        df_totals.insert(1, \"garmin_product\", garmin_product)\n",
    "        # Add the product name to the DataFrame\n",
    "        df_totals.insert(2, \"serial_number\", serial_number)\n",
    "        # Remove brackets\n",
    "        df_totals = df_totals.astype(str)\n",
    "        df_totals = df_totals.apply(lambda x: x.str.replace('[','').str.replace(']', ''), regex=False)\n",
    "\n",
    "        return df_totals\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in get_totals: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### safe_max\n",
    "\n",
    "```markdown\n",
    "Sub function for get_totals: Ensure that values from MAX fields in a list are numeric\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_max(lst):\n",
    "    numeric_lst = [x for x in lst if isinstance(x, (int, float))]\n",
    "    if len(numeric_lst) > 0:\n",
    "        return max(numeric_lst)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is_numeric_field\n",
    "\n",
    "```markdown\n",
    "Sub function for get_totals: Ensure that given fields are numeric\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric_field(field):\n",
    "    return field.value is not None and isinstance(field.value, (int, float))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_user_data\n",
    "\n",
    "```markdown\n",
    "Get pre-defined user data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_data(filename):\n",
    "    try:\n",
    "        # Open the fit file\n",
    "        fitfile = fitparse.FitFile(filename)\n",
    "\n",
    "        # Create an empty list to hold the user data\n",
    "        user_data = []\n",
    "\n",
    "        # Pre-define fields to read from fit file\n",
    "        user_data_fields = [\"activity_class\", \"depth_setting\", \"dist_setting\", \"elev_setting\", \n",
    "                  \"gender\", \"height\", \"height_setting\", \"hr_setting\", \"language\", \n",
    "                  \"position_setting\", \"resting_heart_rate\", \"sleep_time\", \"speed_setting\", \n",
    "                  \"temperature_setting\", \"user_running_step_length\", \"age\",\n",
    "                  \"user_walking_step_length\", \"wake_time\", \"weight\", \"weight_setting\"]\n",
    "\n",
    "        # Loop over all the messages in the FIT file\n",
    "        for record in fitfile.get_messages():\n",
    "            # Create an empty dictionary to hold the user data fields for this message\n",
    "            user_data_fields_dict = {}\n",
    "\n",
    "            # Loop over all the fields in this message\n",
    "            for field in record:\n",
    "                # Check if this field is in the list of user data fields\n",
    "                if field.name in user_data_fields:\n",
    "                    # If so, add it to the dictionary\n",
    "                    if field.name == \"sleep_time\":\n",
    "                        user_data_fields_dict[field.name] = field.value.strftime('%H:%M')\n",
    "                    elif field.name == \"wake_time\":\n",
    "                        user_data_fields_dict[field.name] = field.value.strftime('%H:%M:%S')\n",
    "                    else:\n",
    "                        user_data_fields_dict[field.name] = field.value\n",
    "\n",
    "            # Add the user data fields for this message to the list\n",
    "            if user_data_fields_dict:\n",
    "                user_data.append(user_data_fields_dict)\n",
    "\n",
    "        # Create a pandas dataframe from the user data\n",
    "        df_user_data = pd.DataFrame(user_data)\n",
    "\n",
    "        # Get the base filename without path\n",
    "        filename = os.path.basename(filename)\n",
    "        # Extract the first part of the filename before the first underscore\n",
    "        name_part = filename.split('_')[0]\n",
    "        # Add a new column with the name part of the file name\n",
    "        df_user_data.insert(0, \"activity_number\", name_part)\n",
    "\n",
    "        return df_user_data\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in get_user_data: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_fit_file\n",
    "\n",
    "```markdown\n",
    "Read all data from a single .fit file and write result into pandas dataframe.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fit_file(file_path):\n",
    "    try:\n",
    "        fitfile = fitparse.FitFile(file_path)\n",
    "\n",
    "        data = []\n",
    "        for record in fitfile.get_messages('record'):\n",
    "            # Create an empty dictionary to hold the data for this record\n",
    "            record_data = {}\n",
    "            for data_point in record:\n",
    "                # Convert Garmin-Semicircles to Degree\n",
    "                if data_point.name == 'position_lat' or data_point.name == 'position_long':\n",
    "                    record_data[data_point.name] = semicircles_to_degree(data_point.value)\n",
    "                elif data_point.name == 'radar_speeds':\n",
    "                    record_data[data_point.name] = mph_to_kph(data_point.value)\n",
    "                elif data_point.name == 'passing_speedabs':\n",
    "                    # Include the passing speed in kph directly in the record_data dictionary\n",
    "                    record_data['passing_speed_kph'] = mph_to_kph(data_point.value)\n",
    "                else:\n",
    "                    record_data[data_point.name] = data_point.value\n",
    "\n",
    "            data.append(record_data)\n",
    "\n",
    "        # Convert the list of data to a Pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        # Give the longitude a proper field name\n",
    "        df = df.rename(columns={'position_long': 'position_lon'})\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in read_fit_file: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main-Function: run_fitfile2db\n",
    "\n",
    "```markdown\n",
    "Main function to do all the funny things.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fitfile2db():\n",
    "    try:\n",
    "        # Set the directory to search for .fit files\n",
    "        if fit_path:\n",
    "            directory = fit_path\n",
    "        else:\n",
    "            directory = os.getcwd()\n",
    "\n",
    "        # Find all .fit files in the directory\n",
    "        fit_files = find_fit_files(directory)\n",
    "\n",
    "        # Store filenames in dataframe\n",
    "        df_fn = pd.DataFrame({'filename': fit_files})\n",
    "        fieldname = df_fn.columns[0]\n",
    "        df_fn =df_fn.rename(columns={fieldname: 'filename'})\n",
    "\n",
    "        # Only if db flag is given:\n",
    "        if use_db:\n",
    "            # Check, if filename are known and build a dataframe with new files only\n",
    "            querystring = 'select * from known_fitfiles'\n",
    "            df_known = read_from_database(querystring)\n",
    "            df_fn = df_fn[~df_fn['filename'].isin(df_known['filename'])]\n",
    "            # Exit, if there is no new file\n",
    "            if df_fn.empty:\n",
    "                print('No new files to proceed!')\n",
    "                return\n",
    "        \n",
    "        # Read the data from each (new) .fit file and combine it into a single DataFrame\n",
    "        dfs = []\n",
    "        for fit_file in df_fn['filename']:\n",
    "            df = read_fit_file(fit_file)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            dfs.append(df)\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Convert all fields, except position data, to text\n",
    "        combined_df = combined_df.astype({col: str for col in combined_df.columns if col not in ['position_lat', 'position_lon']})\n",
    "\n",
    "        # Convert position fields to real\n",
    "        if 'position_lat' in combined_df.columns:\n",
    "            combined_df['position_lat'] = pd.to_numeric(combined_df['position_lat'], errors='coerce')\n",
    "        if 'position_lon' in combined_df.columns:\n",
    "            combined_df['position_lon'] = pd.to_numeric(combined_df['position_lon'], errors='coerce')\n",
    "        if 'passing_speed_kph' in combined_df.columns:\n",
    "            combined_df['passing_speed_kph'] = pd.to_numeric(combined_df['passing_speed_kph'], errors='coerce')\n",
    "\n",
    "        # Read the totals from each (new) .fit file and combine it into a single DataFrame\n",
    "        dftotals = []\n",
    "        for fit_file in df_fn['filename']:\n",
    "            dft = get_totals(fit_file)\n",
    "            dft.reset_index(drop=True, inplace=True)\n",
    "            dftotals.append(dft)\n",
    "        combined_df_totals = pd.concat(dftotals, ignore_index=True)\n",
    "\n",
    "        # Read the user data from each (new) .fit file and combine it into a single DataFrame\n",
    "        udtotals = []\n",
    "        for fit_file in df_fn['filename']:\n",
    "            udt = get_user_data(fit_file)\n",
    "            udt.reset_index(drop=True, inplace=True)\n",
    "            udtotals.append(udt)\n",
    "        combined_ud_totals = pd.concat(udtotals, ignore_index=True)\n",
    "\n",
    "        # Write the combined DataFrames into database or to a CSV file\n",
    "        if use_db:\n",
    "            write_to_database(combined_df, 'fitfile_data')\n",
    "            write_to_database(combined_ud_totals, 'fitfile_userdata')\n",
    "            write_to_database(combined_df_totals, 'fitfile_totals')\n",
    "            write_to_database(df_fn, 'known_fitfiles')\n",
    "        elif csv_path:\n",
    "            combined_df.to_csv(f'{csv_path}/{today}_output.csv', index=False)\n",
    "            combined_df_totals.to_csv(f'{csv_path}/{today}_output_totals.csv', index=False)\n",
    "        else:\n",
    "            combined_df.to_csv('output.csv', index=False)\n",
    "            combined_df_totals.to_csv('output_totals.csv', index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(str(e))\n",
    "            print(f\"Error in run_fitfile2db: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fitfile2db()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6e51ffc48c74bc681f1d405acabe6496ba2a44cafe5d9b8a0bf330bfe99020e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
